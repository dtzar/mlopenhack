{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Azure ML API SDK. The SDK is installed implicitly with the latest\n",
    "# version of the CLI in your default python environment\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_image_url = \"https://s7d1.scene7.com/is/image/MoosejawMB/10184925x1011463_zm?$thumb225$\"\n",
    "#test_image_url = \"http://www.ems.com/dw/image/v2/AAQU_PRD/on/demandware.static/-/Sites-vestis-master-catalog/default/dwd70784cc/product/images/2010/536/2010536/2010536_601_main.jpg?sw=195&sh=195&sm=fit\"\n",
    "test_image_url = \"https://ec-i21.geccdn.net/site/images/n-picgroup/DBA_098-2000112.jpg\"\n",
    "#test_image_url = \"https://cdn.shopify.com/s/files/1/1803/7259/files/hammock-rope-carabiner_large.jpg?418721957162400510\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "label_file = \"/home/hacker1/source/tfmodel/output_labels.txt\"\n",
    "model_file = \"/home/hacker1/source/tfmodel/output_graph.pb\"\n",
    "\n",
    "def load_graph(model_file):\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def read_tensor_from_image_file(url, input_height=299, input_width=299,input_mean=0, input_std=255):\n",
    "    im = Image.open(requests.get(url, stream=True).raw)\n",
    "    \n",
    "    im.resize((input_height,input_width), Image.ANTIALIAS)\n",
    "    image_array = np.array(im)[:, :, 0:3]  # Select RGB channels only.\n",
    "    \n",
    "    float_caster = tf.cast(image_array, tf.float32)\n",
    "    dims_expander = tf.expand_dims(float_caster, 0);\n",
    "    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "    sess = tf.Session()\n",
    "    result = sess.run(normalized)\n",
    "\n",
    "    return result\n",
    "\n",
    "def load_labels(label_file):\n",
    "    label = []\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n",
    "    for l in proto_as_ascii_lines:\n",
    "        label.append(l.rstrip())\n",
    "    return label\n",
    "\n",
    "def init():   \n",
    "    # read in the model file\n",
    "    global graph\n",
    "    graph = load_graph(model_file)\n",
    "\n",
    "def run(url):\n",
    "    try:\n",
    "        input_layer = \"Mul\"\n",
    "        output_layer = \"final_result\"\n",
    "        input_name = \"import/\" + input_layer\n",
    "        output_name = \"import/\" + output_layer\n",
    "        input_operation = graph.get_operation_by_name(input_name)\n",
    "        output_operation = graph.get_operation_by_name(output_name)\n",
    "        \n",
    "        t= read_tensor_from_image_file(url)\n",
    "\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            results = sess.run(output_operation.outputs[0], {\n",
    "                input_operation.outputs[0]: t\n",
    "            })\n",
    "            results = np.squeeze(results)\n",
    "\n",
    "        top_k = results.argsort()[-5:][::-1]\n",
    "        labels = load_labels(label_file)\n",
    "        #for i in top_k:\n",
    "        return(str(labels[top_k[0]])+\" \"+str(results[top_k[0]]))\n",
    "    except Exception as e:\n",
    "        return (str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carabiners 0.999843'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import score\n",
    "score.init()\n",
    "score.run(test_image_url)\n",
    "#print(type(test_image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'url': {'internal': 'gANjYXp1cmVtbC5hcGkuc2NoZW1hLnB5dGhvblV0aWwKUHl0aG9uU2NoZW1hCnEAKYFxAX1xAlgJAAAAZGF0YV90eXBlcQNjYnVpbHRpbnMKc3RyCnEEc2Iu',\n",
       "   'swagger': {'example': 'https://ec-i21.geccdn.net/site/images/n-picgroup/DBA_098-2000112.jpg',\n",
       "    'type': 'string'},\n",
       "   'type': 0,\n",
       "   'version': '1.0.1b6'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input dataframe and create the schema\n",
    "url = \"https://ec-i21.geccdn.net/site/images/n-picgroup/DBA_098-2000112.jpg\"\n",
    "inputs = {\"url\": SampleDefinition(DataTypes.STANDARD, url)}\n",
    "generate_schema(run_func=score.run, inputs=inputs, filepath='/home/hacker1/tfmodel/service_schema.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
